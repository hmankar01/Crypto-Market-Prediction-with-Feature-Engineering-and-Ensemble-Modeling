{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96164,"databundleVersionId":12993472,"sourceType":"competition"},{"sourceId":12445188,"sourceType":"datasetVersion","datasetId":7850432}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, InputLayer, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","metadata":{"_uuid":"d3938d79-642f-4e10-beb5-b5e6da7c762c","_cell_guid":"462aff0f-f5a2-41df-b185-d93e46d99482","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T01:59:19.890987Z","iopub.execute_input":"2025-07-16T01:59:19.891354Z","iopub.status.idle":"2025-07-16T01:59:38.847970Z","shell.execute_reply.started":"2025-07-16T01:59:19.891330Z","shell.execute_reply":"2025-07-16T01:59:38.846558Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))","metadata":{"_uuid":"c815259b-82d2-47f6-9298-2f1854c1532c","_cell_guid":"b13db38e-741a-4ef4-9a9c-3cdd377b08cf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T01:59:38.850012Z","iopub.execute_input":"2025-07-16T01:59:38.850760Z","iopub.status.idle":"2025-07-16T01:59:38.860157Z","shell.execute_reply.started":"2025-07-16T01:59:38.850729Z","shell.execute_reply":"2025-07-16T01:59:38.858951Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')","metadata":{"_uuid":"63c06c98-988c-403a-b5aa-c659c7ff05be","_cell_guid":"4e20b74e-d2f4-4176-aead-c50bc2ad5e27","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T01:59:38.861469Z","iopub.execute_input":"2025-07-16T01:59:38.861853Z","iopub.status.idle":"2025-07-16T02:00:05.949012Z","shell.execute_reply.started":"2025-07-16T01:59:38.861820Z","shell.execute_reply":"2025-07-16T02:00:05.947806Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"_uuid":"b6c3d58c-ae8b-4994-aaf5-5d406371bdda","_cell_guid":"21bbaee6-41d1-4860-b7c1-5289ecafb6fb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:00:05.951072Z","iopub.execute_input":"2025-07-16T02:00:05.951347Z","iopub.status.idle":"2025-07-16T02:00:05.959799Z","shell.execute_reply.started":"2025-07-16T02:00:05.951325Z","shell.execute_reply":"2025-07-16T02:00:05.958798Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add a small constant to prevent division by zero\nepsilon = 1e-10\n\n# --- Imbalances & Spreads ---\n# Captures the immediate pressure on the order book\ndf['order_book_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + epsilon)\n\n# Captures the pressure from market orders\ndf['trade_imbalance'] = (df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'] + epsilon)\n\n# Simple difference in quoted quantities\ndf['quantity_spread'] = df['ask_qty'] - df['bid_qty']\n\n\n# --- Sizes & Proxies ---\n# Average size of trades in that minute\ndf['avg_trade_size'] = df['volume'] / (df['buy_qty'] + df['sell_qty'] + epsilon)\n\n# Your WAP proxy - a measure of price pressure based on order book depth\ndf['wap_proxy'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + epsilon)\n\n\n# --- Log-Transformed Features ---\n# Reduces skewness in features with large values\ndf['log_volume'] = np.log1p(df['volume'])\ndf['log_ask_qty'] = np.log1p(df['ask_qty'])\ndf['log_bid_qty'] = np.log1p(df['bid_qty'])\n\n\n# --- Interaction Feature ---\n# Combines imbalance with total volume\ndf['imbalance_times_volume'] = df['order_book_imbalance'] * df['volume']\n\n# Display the new columns to verify\nprint(df.columns)","metadata":{"_uuid":"cf45be82-807d-499b-b40b-41701db6f4e4","_cell_guid":"d893e20d-7e5f-48f1-ad79-51abc12ebe37","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:00:05.960800Z","iopub.execute_input":"2025-07-16T02:00:05.961109Z","iopub.status.idle":"2025-07-16T02:00:06.069572Z","shell.execute_reply.started":"2025-07-16T02:00:05.961085Z","shell.execute_reply":"2025-07-16T02:00:06.067926Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_cols = [f'X{i}' for i in range(1, 781)]\ndf['X_mean'] = df[x_cols].mean(axis=1)\ndf['X_std'] = df[x_cols].std(axis=1)\ndf['X_skew'] = df[x_cols].skew(axis=1)\ndf = df.copy()","metadata":{"_uuid":"a348dc3f-40f8-43b2-a19c-981e5e785284","_cell_guid":"24496f95-b2e8-4cd6-9d8e-be8a35607bf0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:00:06.070705Z","iopub.execute_input":"2025-07-16T02:00:06.070999Z","iopub.status.idle":"2025-07-16T02:00:34.819564Z","shell.execute_reply.started":"2025-07-16T02:00:06.070977Z","shell.execute_reply":"2025-07-16T02:00:34.817839Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['X_n_positive'] = (df[x_cols] > 0).sum(axis=1)\ndf['X_n_negative'] = (df[x_cols] < 0).sum(axis=1)","metadata":{"_uuid":"b1c99ce4-e66e-4478-8aa6-0b8693991b3b","_cell_guid":"6f6553ac-4041-4d63-b561-941307e3c563","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:00:34.821661Z","iopub.execute_input":"2025-07-16T02:00:34.822049Z","iopub.status.idle":"2025-07-16T02:00:40.379564Z","shell.execute_reply.started":"2025-07-16T02:00:34.822012Z","shell.execute_reply":"2025-07-16T02:00:40.378706Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quantiles provide a robust summary of the distribution\ndf['X_median'] = df[x_cols].median(axis=1)\ndf['X_q25'] = df[x_cols].quantile(0.25, axis=1)\ndf['X_q75'] = df[x_cols].quantile(0.75, axis=1)\n\n# Kurtosis measures the \"tailedness\" (presence of outliers)\ndf['X_kurtosis'] = df[x_cols].kurtosis(axis=1)","metadata":{"_uuid":"7e185b38-a185-4142-a783-647835c203b1","_cell_guid":"d1f570a0-a022-42f3-b900-e47fb8fd30e6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:00:40.380609Z","iopub.execute_input":"2025-07-16T02:00:40.380905Z","iopub.status.idle":"2025-07-16T02:01:19.107308Z","shell.execute_reply.started":"2025-07-16T02:00:40.380881Z","shell.execute_reply":"2025-07-16T02:01:19.106330Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# A small constant to prevent division by zero\nepsilon = 1e-10\n\n# What proportion of the minute's total volume was from aggressive buyers?\ndf['buy_proportion'] = df['buy_qty'] / (df['volume'] + epsilon)\n\n# A simple measure of overall market depth/liquidity on the books\ndf['total_depth'] = df['bid_qty'] + df['ask_qty']","metadata":{"_uuid":"8eeccf5b-3ffc-4083-b910-92fda13da5c3","_cell_guid":"79740b11-1669-48b7-8787-8a6740b93038","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:19.108395Z","iopub.execute_input":"2025-07-16T02:01:19.108746Z","iopub.status.idle":"2025-07-16T02:01:19.120294Z","shell.execute_reply.started":"2025-07-16T02:01:19.108711Z","shell.execute_reply":"2025-07-16T02:01:19.119381Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Polynomial Features ---\n\n# Squaring key features can help capture non-linear effects\ndf['wap_proxy_sq'] = df['wap_proxy']**2\ndf['X_mean_sq'] = df['X_mean']**2","metadata":{"_uuid":"4c88b732-59c4-4b07-8383-edcc867900f0","_cell_guid":"3ec67be6-99ba-41d9-bbef-e295095d57d6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:19.124493Z","iopub.execute_input":"2025-07-16T02:01:19.124830Z","iopub.status.idle":"2025-07-16T02:01:19.149658Z","shell.execute_reply.started":"2025-07-16T02:01:19.124807Z","shell.execute_reply":"2025-07-16T02:01:19.148641Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epsilon = 1e-10\n\n# --- Market Activity Intensity ---\n# Measures how much of the standing liquidity on the book was consumed by trades.\n# A high value might indicate a significant market-moving event.\ndf['activity_intensity'] = df['volume'] / (df['bid_qty'] + df['ask_qty'] + epsilon)\n\n# --- Imbalance Delta ---\n# The difference between executed trade pressure and quoted book pressure.\n# A large value suggests aggressive actors are overwhelming the passive ones.\ndf['imbalance_delta'] = df['trade_imbalance'] - df['order_book_imbalance']","metadata":{"_uuid":"21d1581a-dc27-41f9-a104-fa8fcb012765","_cell_guid":"62610e70-019f-432f-a219-227e3cd0f61c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:19.150796Z","iopub.execute_input":"2025-07-16T02:01:19.151133Z","iopub.status.idle":"2025-07-16T02:01:19.164683Z","shell.execute_reply.started":"2025-07-16T02:01:19.151100Z","shell.execute_reply":"2025-07-16T02:01:19.163775Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# You should have already created 'trade_imbalance' and 'total_depth'\ndf['flow_depth_interaction'] = df['trade_imbalance'] * df['total_depth']","metadata":{"_uuid":"4faaaadc-c05f-47c8-bebe-6baa468ef017","_cell_guid":"328d1626-0796-4b13-8122-a4788bb18466","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:19.165632Z","iopub.execute_input":"2025-07-16T02:01:19.165922Z","iopub.status.idle":"2025-07-16T02:01:19.183281Z","shell.execute_reply.started":"2025-07-16T02:01:19.165899Z","shell.execute_reply":"2025-07-16T02:01:19.182366Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define groups of X features\nn_groups = 5\ngroup_size = len(x_cols) // n_groups # 780 / 5 = 156\n\nfor i in range(n_groups):\n    start_index = i * group_size\n    end_index = start_index + group_size\n    group_cols = x_cols[start_index:end_index]\n    \n    # Create mean and std for each group\n    df[f'X_group_{i+1}_mean'] = df[group_cols].mean(axis=1)\n    df[f'X_group_{i+1}_std'] = df[group_cols].std(axis=1)","metadata":{"_uuid":"8640de8c-bbbb-43b7-a44d-1682a8eee402","_cell_guid":"3d9b8ee4-185a-4173-86c3-7bf9b0e48e3a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:19.184217Z","iopub.execute_input":"2025-07-16T02:01:19.184489Z","iopub.status.idle":"2025-07-16T02:01:26.246262Z","shell.execute_reply.started":"2025-07-16T02:01:19.184469Z","shell.execute_reply":"2025-07-16T02:01:26.244800Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_values = df[x_cols].values\n\n# Calculate the number of times the sign of the signal changes\ndf['X_zero_crossings'] = np.sum(np.diff(np.sign(x_values), axis=1) != 0, axis=1)","metadata":{"_uuid":"b0ceb79f-10f1-4e4e-b3b7-8f21a389ba0b","_cell_guid":"9f302677-a273-4450-afc3-9c4f8a116560","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:26.247357Z","iopub.execute_input":"2025-07-16T02:01:26.247678Z","iopub.status.idle":"2025-07-16T02:01:30.631256Z","shell.execute_reply.started":"2025-07-16T02:01:26.247655Z","shell.execute_reply":"2025-07-16T02:01:30.630184Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downcast integer columns\nint_cols = df.select_dtypes(include=['int64']).columns\nfor col in int_cols:\n    df[col] = pd.to_numeric(df[col], downcast='integer')\n\n# Downcast float columns\nfloat_cols = df.select_dtypes(include=['float64']).columns\nfor col in float_cols:\n    df[col] = pd.to_numeric(df[col], downcast='float')\n\nprint(\"Memory usage after downcasting:\")\ndf.info(memory_usage='deep')","metadata":{"_uuid":"e5ce0bef-c90f-49ab-a900-b5f3b366bca0","_cell_guid":"9071d52a-fe4c-44c8-91e5-48065fda7a11","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:30.632297Z","iopub.execute_input":"2025-07-16T02:01:30.632599Z","iopub.status.idle":"2025-07-16T02:01:43.658757Z","shell.execute_reply.started":"2025-07-16T02:01:30.632578Z","shell.execute_reply":"2025-07-16T02:01:43.657607Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"9128ac3d-ba34-48f5-bd76-5e4427f69da5","_cell_guid":"b60ac236-0892-4109-bd7f-38e7e7d67e51","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:43.660021Z","iopub.execute_input":"2025-07-16T02:01:43.660377Z","iopub.status.idle":"2025-07-16T02:01:43.719684Z","shell.execute_reply.started":"2025-07-16T02:01:43.660347Z","shell.execute_reply":"2025-07-16T02:01:43.718707Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.reset_index(inplace=True)\ndf.rename(columns={'index': 'timestamp'}, inplace=True)\nprint(df.columns)\ndf.head()","metadata":{"_uuid":"8b468d85-74b3-4461-a5c2-c287eb4dccd2","_cell_guid":"2e1b4e9f-26e4-443a-bc0a-ea095c0e87b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:43.720776Z","iopub.execute_input":"2025-07-16T02:01:43.721144Z","iopub.status.idle":"2025-07-16T02:01:43.771609Z","shell.execute_reply.started":"2025-07-16T02:01:43.721111Z","shell.execute_reply":"2025-07-16T02:01:43.769973Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Step 2: Define the validation period (February 2024, as planned)\nvalidation_start_date = pd.to_datetime('2024-02-01')\n\n# Step 3: Split the data into training and validation sets\ntrain_df = df[df['timestamp'] < validation_start_date].copy()\nval_df = df[df['timestamp'] >= validation_start_date].copy()\n\n# Step 4: Separate features (X) from the target (y)\n# Create a list of all feature columns to use\nfeatures = [col for col in train_df.columns if col not in ['label', 'timestamp']]\n\nX_train = train_df[features]\ny_train = train_df['label']\n\nX_val = val_df[features]\ny_val = val_df['label']\n\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Validation set shape: {X_val.shape}\")","metadata":{"_uuid":"69416ee3-9c43-4da2-ad53-f163f969aee9","_cell_guid":"ded79eed-58a2-4a80-a7c4-4266084643c6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:43.772829Z","iopub.execute_input":"2025-07-16T02:01:43.773137Z","iopub.status.idle":"2025-07-16T02:01:51.126098Z","shell.execute_reply.started":"2025-07-16T02:01:43.773115Z","shell.execute_reply":"2025-07-16T02:01:51.124647Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 1: Learn thresholds ONLY from the training data ---\nhigh_volume_threshold = train_df['volume'].quantile(0.95)\nhigh_x_std_threshold = train_df['X_std'].quantile(0.95)\n\nprint(f\"High Volume Threshold (95th percentile): {high_volume_threshold}\")\nprint(f\"High X_std Threshold (95th percentile): {high_x_std_threshold}\")\n\n# --- Step 2: Create the new flag features on both datasets ---\n# Use .copy() to avoid SettingWithCopyWarning\nX_train = X_train.copy()\nX_val = X_val.copy()\n\nX_train['is_high_volume'] = (train_df['volume'] > high_volume_threshold).astype(int)\nX_train['is_high_x_volatility'] = (train_df['X_std'] > high_x_std_threshold).astype(int)\n\nX_val['is_high_volume'] = (val_df['volume'] > high_volume_threshold).astype(int)\nX_val['is_high_x_volatility'] = (val_df['X_std'] > high_x_std_threshold).astype(int)\n\n\nprint(\"\\nNew feature shapes:\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}\")","metadata":{"_uuid":"20dd4073-dfc0-43a5-b98c-c7577ba2f310","_cell_guid":"365c70d1-9054-45ad-a415-2dee8d50227b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:51.127601Z","iopub.execute_input":"2025-07-16T02:01:51.127971Z","iopub.status.idle":"2025-07-16T02:01:51.798079Z","shell.execute_reply.started":"2025-07-16T02:01:51.127938Z","shell.execute_reply":"2025-07-16T02:01:51.797234Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Initialize the scaler\nscaler = StandardScaler()\n\n# Step 2: Fit the scaler ONLY on the training feature data\nscaler.fit(X_train)\n\n# Step 3: Transform your training and validation sets\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n\n# The output of this step, X_train_scaled and X_val_scaled, are NumPy arrays.\n# This is the format your model will expect.\nprint(\"Data scaling complete. Ready for model building.\")","metadata":{"_uuid":"ae3d2f83-ac23-421c-9a32-ee7cc9f84089","_cell_guid":"3c690c7a-501d-4b1d-8fca-6559db732e24","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:01:51.799154Z","iopub.execute_input":"2025-07-16T02:01:51.799471Z","iopub.status.idle":"2025-07-16T02:02:00.341579Z","shell.execute_reply.started":"2025-07-16T02:01:51.799447Z","shell.execute_reply":"2025-07-16T02:02:00.340617Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Custom Loss Function to Maximize Pearson Correlation ---\n# The goal is to minimize the negative Pearson correlation.\ndef pearson_loss(y_true, y_pred):\n    y_true_mean = tf.reduce_mean(y_true)\n    y_pred_mean = tf.reduce_mean(y_pred)\n    y_true_centered = y_true - y_true_mean\n    y_pred_centered = y_pred - y_pred_mean\n    \n    # Add a small epsilon to prevent division by zero\n    corr = (tf.reduce_sum(y_true_centered * y_pred_centered) /\n            (tf.sqrt(tf.reduce_sum(tf.square(y_true_centered))) *\n             tf.sqrt(tf.reduce_sum(tf.square(y_pred_centered))) + 1e-7))\n    \n    return -corr # We minimize the negative correlation","metadata":{"_uuid":"01d25e95-8e37-467f-8abf-11ff46084f25","_cell_guid":"6ecd9893-c04d-4568-8130-4163920aa091","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:02:00.342862Z","iopub.execute_input":"2025-07-16T02:02:00.343289Z","iopub.status.idle":"2025-07-16T02:02:00.349551Z","shell.execute_reply.started":"2025-07-16T02:02:00.343265Z","shell.execute_reply":"2025-07-16T02:02:00.348618Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom scipy.stats import pearsonr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Initialize and train the LightGBM Regressor ---\nlgb_model = lgb.LGBMRegressor(\n    objective='regression_l1', # L1 loss is robust for this kind of data\n    n_estimators=2000,         # High number, but we'll use early stopping\n    learning_rate=0.01,\n    num_leaves=40,\n    n_jobs=-1,                 # Use all available CPU cores\n    seed=42,\n    colsample_bytree=0.7,      # Use a random subset of features for each tree\n    subsample=0.7              # Use a random subset of data for each tree\n)\n\nprint(\"--- Training LightGBM model ---\")\nlgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='l1',\n    callbacks=[lgb.early_stopping(100, verbose=True)] # Stop after 100 rounds of no improvement\n)\n\n# --- Evaluate the LightGBM model ---\nlgb_predictions = lgb_model.predict(X_val)\ncorrelation_lgb, _ = pearsonr(y_val, lgb_predictions)\nprint(f\"\\nLightGBM Pearson Correlation on Validation Set: {correlation_lgb:.6f}\")","metadata":{"_uuid":"593b17a2-f049-41d5-91aa-1d49cfb3054d","_cell_guid":"fbec388b-86df-4049-abb9-a7c44cd8c400","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:02:09.854982Z","iopub.execute_input":"2025-07-16T02:02:09.855304Z","iopub.status.idle":"2025-07-16T02:05:18.280272Z","shell.execute_reply.started":"2025-07-16T02:02:09.855282Z","shell.execute_reply":"2025-07-16T02:05:18.278584Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Get and Plot Feature Importance ---\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': lgb_model.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\nsns.barplot(x='importance', y='feature', data=importance_df.head(30))\nplt.title('Top 30 Most Important Features from LightGBM')\nplt.show()\n\n# --- Select the Top 200 Features ---\nN_TOP_FEATURES = 200\ntop_features = importance_df.head(N_TOP_FEATURES)['feature'].tolist()\n\n# Create new DataFrames with only the top features\nX_train_top = X_train[top_features]\nX_val_top = X_val[top_features]\n\n# We need to re-scale the data now that we have a different set of columns\nscaler_top = StandardScaler()\nX_train_top_scaled = scaler_top.fit_transform(X_train_top)\nX_val_top_scaled = scaler_top.transform(X_val_top)\n\nprint(f\"\\nSelected top {N_TOP_FEATURES} features. New training shape: {X_train_top_scaled.shape}\")","metadata":{"_uuid":"81852d19-9e73-4115-a697-3ac7233ad296","_cell_guid":"78c2525c-465f-4386-841d-53c13e736659","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:05:18.281896Z","iopub.execute_input":"2025-07-16T02:05:18.283433Z","iopub.status.idle":"2025-07-16T02:05:21.127597Z","shell.execute_reply.started":"2025-07-16T02:05:18.283374Z","shell.execute_reply":"2025-07-16T02:05:21.126399Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Define a simpler MLP architecture ---\nn_features_top = X_train_top_scaled.shape[1]\n\nmlp_model = Sequential([\n    InputLayer(input_shape=(n_features_top,)),\n    BatchNormalization(),\n    Dense(256, activation='relu'),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(128, activation='relu'),\n    Dropout(0.4),\n    Dense(1, activation='linear')\n])\n\noptimizer = Adam(learning_rate=0.0005)\nmlp_model.compile(optimizer=optimizer, loss=pearson_loss)\n\n# --- Define Callbacks and Train ---\ncheckpoint_mlp = ModelCheckpoint('best_mlp_model.keras', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_mlp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\nearly_stopping_mlp = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n\nprint(\"\\n--- Training Refined MLP model on Top Features ---\")\nhistory_mlp = mlp_model.fit(\n    X_train_top_scaled, y_train,\n    validation_data=(X_val_top_scaled, y_val),\n    epochs=100,\n    batch_size=1024,\n    callbacks=[checkpoint_mlp, reduce_lr_mlp, early_stopping_mlp],\n    verbose=1\n)\n\n# --- Evaluate the refined MLP model ---\nmlp_predictions = mlp_model.predict(X_val_top_scaled).flatten()\ncorrelation_mlp, _ = pearsonr(y_val, mlp_predictions)\nprint(f\"\\nRefined MLP Pearson Correlation on Validation Set: {correlation_mlp:.6f}\")","metadata":{"_uuid":"27b9559d-053f-47b6-926c-d0464d4bd113","_cell_guid":"94bbf845-10ec-4ecf-9f0e-05d4c9451215","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:06:20.694025Z","iopub.execute_input":"2025-07-16T02:06:20.694424Z","iopub.status.idle":"2025-07-16T02:09:14.207311Z","shell.execute_reply.started":"2025-07-16T02:06:20.694399Z","shell.execute_reply":"2025-07-16T02:09:14.206236Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Create the Ensemble Prediction ---\n# We already have lgb_predictions and mlp_predictions\nensemble_predictions = (lgb_predictions * 0.5) + (mlp_predictions * 0.5)\n\n# --- Evaluate the Ensemble ---\ncorrelation_ensemble, _ = pearsonr(y_val, ensemble_predictions)\nprint(f\"\\nEnsemble Pearson Correlation on Validation Set: {correlation_ensemble:.6f}\")","metadata":{"_uuid":"5c3ea6c3-462b-4bdb-85a7-5ab935cf8eec","_cell_guid":"990cf33b-d3cf-4145-9daf-0048f2e7b16c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:10:58.511248Z","iopub.execute_input":"2025-07-16T02:10:58.511677Z","iopub.status.idle":"2025-07-16T02:10:58.520619Z","shell.execute_reply.started":"2025-07-16T02:10:58.511650Z","shell.execute_reply":"2025-07-16T02:10:58.519617Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom scipy.stats import pearsonr\nimport lightgbm as lgb\n\ndef objective(trial):\n    \"\"\"Define the objective function for Optuna to optimize.\"\"\"\n    \n    # Define the search space for hyperparameters\n    params = {\n        'objective': 'regression_l1',\n        'metric': 'l1',\n        'n_estimators': 2000,\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        'n_jobs': -1,\n        'seed': 42,\n        'boosting_type': 'gbdt',\n    }\n    \n    # Train the model with the suggested params\n    model = lgb.LGBMRegressor(**params)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='l1',\n        callbacks=[lgb.early_stopping(100, verbose=False)]\n    )\n    \n    # Make predictions and calculate Pearson correlation\n    preds = model.predict(X_val)\n    # We want to maximize correlation, so Optuna will maximize this value\n    pearson_corr, _ = pearsonr(y_val, preds)\n    \n    return pearson_corr\n\n# --- Start the Optuna study ---\n# We want to 'maximize' the Pearson correlation\nstudy = optuna.create_study(direction='maximize')\n# Run 30 trials to find the best params. You can increase this for a more thorough search.\nstudy.trials_dataframe().to_csv('tuner_results.csv')\nstudy.optimize(objective, n_trials=30)\n\n# --- Print the best results ---\nprint(\"\\nBest trial:\")\ntrial = study.best_trial\nprint(f\"  Value (Pearson Correlation): {trial.value:.6f}\")\nprint(\"  Best hyperparameters: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\n# --- Train the final best model with the optimal parameters ---\nbest_params = trial.params\nfinal_lgb_model = lgb.LGBMRegressor(n_estimators=2000, **best_params)\nfinal_lgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='l1',\n    callbacks=[lgb.early_stopping(100)]\n)","metadata":{"_uuid":"5b30887b-8542-4335-92d3-1a0084c45264","_cell_guid":"5ec310e5-7495-429b-8840-d1d133ca03c3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T02:13:00.533197Z","iopub.execute_input":"2025-07-16T02:13:00.533659Z","iopub.status.idle":"2025-07-16T04:10:52.902881Z","shell.execute_reply.started":"2025-07-16T02:13:00.533631Z","shell.execute_reply":"2025-07-16T04:10:52.900204Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Get predictions from your best models ---\ntuned_lgb_preds = final_lgb_model.predict(X_val)\n# mlp_predictions should still be stored from your previous run\n\n# --- Create a weighted ensemble ---\n# Give more weight to the stronger model (LGBM)\nweighted_ensemble_preds = (tuned_lgb_preds * 0.7) + (mlp_predictions * 0.3)\n\n# --- Evaluate the new ensemble ---\ncorrelation_weighted_ensemble, _ = pearsonr(y_val, weighted_ensemble_preds)\nprint(f\"\\nWeighted Ensemble Pearson Correlation: {correlation_weighted_ensemble:.6f}\")","metadata":{"_uuid":"08b3143f-f5c4-427e-bacb-09d0184d85c6","_cell_guid":"b57d6a3f-967b-4bfb-8ed0-bd726e2471d0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T04:12:43.805171Z","iopub.execute_input":"2025-07-16T04:12:43.805715Z","iopub.status.idle":"2025-07-16T04:12:44.064786Z","shell.execute_reply.started":"2025-07-16T04:12:43.805611Z","shell.execute_reply":"2025-07-16T04:12:44.063802Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom scipy.stats import pearsonr\n\n# --- Get the best hyperparameters from your completed study ---\nbest_params = study.best_trial.params\n\n# --- Train the final model with a fixed number of iterations ---\n# We are removing the early stopping callback for this final fit.\nprint(\"Training the final, tuned LightGBM model...\")\nfinal_lgb_model = lgb.LGBMRegressor(\n    n_estimators=400, # Train for a fixed 400 rounds\n    **best_params     # Use the best params found by Optuna\n)\n\n# Fit the model on the full training data\nfinal_lgb_model.fit(X_train, y_train)\n\n# --- Get predictions from the PROPERLY trained model ---\ntuned_lgb_preds = final_lgb_model.predict(X_val)\ncorrelation_tuned_lgb, _ = pearsonr(y_val, tuned_lgb_preds)\nprint(f\"\\nCorrectly Trained LightGBM Correlation: {correlation_tuned_lgb:.6f}\")","metadata":{"_uuid":"9bd0fbaa-1e6e-4fdf-9333-cb93065331df","_cell_guid":"044ae315-8731-40cf-93c3-dcad91b5dd80","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T04:15:17.305800Z","iopub.execute_input":"2025-07-16T04:15:17.306271Z","iopub.status.idle":"2025-07-16T04:18:35.037234Z","shell.execute_reply.started":"2025-07-16T04:15:17.306234Z","shell.execute_reply":"2025-07-16T04:18:35.036322Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mlp_predictions should still be stored from your previous MLP run\n\n# --- Create the new weighted ensemble ---\nweighted_ensemble_preds = (tuned_lgb_preds * 0.7) + (mlp_predictions * 0.3)\n\n# --- Evaluate the new ensemble ---\ncorrelation_weighted_ensemble, _ = pearsonr(y_val, weighted_ensemble_preds)\nprint(f\"Final Weighted Ensemble Pearson Correlation: {correlation_weighted_ensemble:.6f}\")","metadata":{"_uuid":"217fb015-c6d2-465f-9713-427cac427828","_cell_guid":"0cdc1390-cc33-4db2-897b-7976859445da","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T04:21:53.232115Z","iopub.execute_input":"2025-07-16T04:21:53.232376Z","iopub.status.idle":"2025-07-16T04:21:53.241913Z","shell.execute_reply.started":"2025-07-16T04:21:53.232355Z","shell.execute_reply":"2025-07-16T04:21:53.240850Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import pearsonr\n\n# We'll use your original training dataframe before splitting\n# Ensure it contains all original and engineered features\n# Let's call it `full_train_df` for this example. Assuming you have `train_df` ready.\n\nprint(\"Calculating correlation of all features with the label...\")\n# This computes the Pearson correlation of every column with 'label'\n# .abs() gets the absolute correlation, as a strong negative one is also good\ncorrelations = train_df.corr(method='pearson')['label'].abs().sort_values(ascending=False)\n\nprint(\"\\n--- Top 20 Features Most Correlated with 'label' ---\")\nprint(correlations.head(20))\n\n# --- Test the single best feature as a prediction ---\n# The top feature will be 'label' itself (corr=1.0), so we take the second one\ngolden_feature_name = correlations.index[1]\nprint(f\"\\nThe most likely 'Golden Feature' is: {golden_feature_name}\")\n\n# Let's see what score we get by using only this feature's values as our prediction\n# We will use the validation set for this test\ngolden_feature_predictions = val_df[golden_feature_name]\ncorrelation_golden, _ = pearsonr(y_val, golden_feature_predictions)\n\nprint(f\"\\nCorrelation score using ONLY '{golden_feature_name}' as prediction: {correlation_golden:.6f}\")","metadata":{"_uuid":"d0c107cb-43f0-46bb-8715-5d17f4673b12","_cell_guid":"c2db35c5-e7d6-403f-9ce1-e68e7ddcbe0c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T04:34:57.103593Z","iopub.execute_input":"2025-07-16T04:34:57.103979Z","iopub.status.idle":"2025-07-16T04:49:48.405829Z","shell.execute_reply.started":"2025-07-16T04:34:57.103957Z","shell.execute_reply":"2025-07-16T04:49:48.403376Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\ntest_df.head()","metadata":{"_uuid":"9f6d2555-7158-4d5f-b2c3-aba2e7d13652","_cell_guid":"b8d9aec5-8841-4a55-bd4b-db493ebb00e7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T05:08:20.181483Z","iopub.execute_input":"2025-07-16T05:08:20.181843Z","iopub.status.idle":"2025-07-16T05:08:20.280145Z","shell.execute_reply.started":"2025-07-16T05:08:20.181814Z","shell.execute_reply":"2025-07-16T05:08:20.278749Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# --- 1. Load the test data ---\nprint(\"Loading test data...\")\ntest_df = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\ntest_ids = test_df['ID']\n\n\n# --- 2. Apply ALL Feature Engineering Steps ---\nprint(\"Applying feature engineering to the test set...\")\nepsilon = 1e-10\n\n# Imbalances, Spreads, Sizes, Proxies, Logs, and basic interactions\ntest_df['order_book_imbalance'] = (test_df['bid_qty'] - test_df['ask_qty']) / (test_df['bid_qty'] + test_df['ask_qty'] + epsilon)\ntest_df['trade_imbalance'] = (test_df['buy_qty'] - test_df['sell_qty']) / (test_df['buy_qty'] + test_df['sell_qty'] + epsilon)\ntest_df['quantity_spread'] = test_df['ask_qty'] - test_df['bid_qty']\ntest_df['avg_trade_size'] = test_df['volume'] / (test_df['buy_qty'] + test_df['sell_qty'] + epsilon)\ntest_df['wap_proxy'] = (test_df['bid_qty'] - test_df['ask_qty']) / (test_df['bid_qty'] + test_df['ask_qty'] + epsilon)\ntest_df['log_volume'] = np.log1p(test_df['volume'])\ntest_df['log_ask_qty'] = np.log1p(test_df['ask_qty'])\ntest_df['log_bid_qty'] = np.log1p(test_df['bid_qty'])\ntest_df['imbalance_times_volume'] = test_df['order_book_imbalance'] * test_df['volume']\n\n# X-feature statistics\ntest_df['X_mean'] = test_df[x_cols].mean(axis=1)\ntest_df['X_std'] = test_df[x_cols].std(axis=1)\ntest_df['X_skew'] = test_df[x_cols].skew(axis=1)\ntest_df['X_n_positive'] = (test_df[x_cols] > 0).sum(axis=1)\ntest_df['X_n_negative'] = (test_df[x_cols] < 0).sum(axis=1)\ntest_df['X_median'] = test_df[x_cols].median(axis=1)\ntest_df['X_q25'] = test_df[x_cols].quantile(0.25, axis=1)\ntest_df['X_q75'] = test_df[x_cols].quantile(0.75, axis=1)\ntest_df['X_kurtosis'] = test_df[x_cols].kurtosis(axis=1)\n\n# Advanced ratios, interactions, and polynomial features\ntest_df['buy_proportion'] = test_df['buy_qty'] / (test_df['volume'] + epsilon)\ntest_df['total_depth'] = test_df['bid_qty'] + test_df['ask_qty']\ntest_df['wap_proxy_sq'] = test_df['wap_proxy']**2\ntest_df['X_mean_sq'] = test_df['X_mean']**2\ntest_df['activity_intensity'] = test_df['volume'] / (test_df['bid_qty'] + test_df['ask_qty'] + epsilon)\ntest_df['imbalance_delta'] = test_df['trade_imbalance'] - test_df['order_book_imbalance']\ntest_df['flow_depth_interaction'] = test_df['trade_imbalance'] * test_df['total_depth']\n\n# X-group and X-sequence features\nn_groups = 5\ngroup_size = len(x_cols) // n_groups\nfor i in range(n_groups):\n    start_index = i * group_size\n    end_index = start_index + group_size\n    group_cols = x_cols[start_index:end_index]\n    test_df[f'X_group_{i+1}_mean'] = test_df[group_cols].mean(axis=1)\n    test_df[f'X_group_{i+1}_std'] = test_df[group_cols].std(axis=1)\ntest_df['X_zero_crossings'] = np.sum(np.diff(np.sign(test_df[x_cols].values), axis=1) != 0, axis=1)\n\n\n# --- 3. Predict with Tuned LightGBM ---\nprint(\"Predicting with LightGBM...\")\n# Ensure test_df columns are in the same order as X_train for the model\nlgb_test_preds = final_lgb_model.predict(test_df[X_train.columns])\n\n\n# --- 4. Predict with Refined MLP ---\nprint(\"Predicting with MLP...\")\n# Select the same top features for the MLP\nX_test_top = test_df[top_features]\n# Use the scaler that was FITTED ON THE TRAINING DATA to transform the test data\nX_test_top_scaled = scaler_top.transform(X_test_top)\n# The mlp_model is the one trained on top features\nmlp_test_preds = mlp_model.predict(X_test_top_scaled).flatten()\n\n\n# --- 5. Create the Final Weighted Ensemble ---\nprint(\"Creating weighted ensemble...\")\nfinal_test_predictions = (lgb_test_preds * 0.7) + (mlp_test_preds * 0.3)\n\n\n# --- 6. Generate the submission file ---\nprint(\"Creating submission file...\")\nsubmission_df = pd.DataFrame({'ID': test_ids, 'label': final_test_predictions})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\n'submission.csv' has been created successfully!\")","metadata":{"_uuid":"2afcff4f-45d6-4258-840b-5e7d8e766614","_cell_guid":"a55dabb3-e4d2-4c9b-9127-481e8f19b8c6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-16T05:07:07.291301Z","iopub.execute_input":"2025-07-16T05:07:07.292366Z","iopub.status.idle":"2025-07-16T05:07:35.605216Z","shell.execute_reply.started":"2025-07-16T05:07:07.292323Z","shell.execute_reply":"2025-07-16T05:07:35.602665Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"3029c525-94af-4a2a-8080-2f4d8ea74b97","_cell_guid":"8dab6cfd-91b5-4e14-a276-bc28c612dc90","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}