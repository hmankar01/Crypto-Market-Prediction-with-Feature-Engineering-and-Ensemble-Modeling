{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11cefad6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-17T10:19:58.334048Z",
     "iopub.status.busy": "2025-07-17T10:19:58.333267Z",
     "iopub.status.idle": "2025-07-17T14:10:13.714297Z",
     "shell.execute_reply": "2025-07-17T14:10:13.709348Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 13815.391314,
     "end_time": "2025-07-17T14:10:13.720058",
     "exception": false,
     "start_time": "2025-07-17T10:19:58.328744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading engineered train data...\n",
      "Loading and engineering test data...\n",
      "Applying feature engineering...\n",
      "\n",
      "--- Starting LightGBM Training & Prediction ---\n",
      "LGBM Fold 1/5\n",
      "Saved model to /kaggle/working/lgbm_models/lgbm_fold_1.txt\n",
      "LGBM Fold 2/5\n",
      "Saved model to /kaggle/working/lgbm_models/lgbm_fold_2.txt\n",
      "LGBM Fold 3/5\n",
      "Saved model to /kaggle/working/lgbm_models/lgbm_fold_3.txt\n",
      "LGBM Fold 4/5\n",
      "Saved model to /kaggle/working/lgbm_models/lgbm_fold_4.txt\n",
      "LGBM Fold 5/5\n",
      "Saved model to /kaggle/working/lgbm_models/lgbm_fold_5.txt\n",
      "\n",
      "LGBM OOF Pearson Correlation: 0.97070\n",
      "Saving top 250 feature list to /kaggle/working/sorted_features.csv\n",
      "Saving LGBM OOF predictions to /kaggle/working/oof_lgbm.npy\n",
      "Saving LGBM Test predictions to /kaggle/working/test_preds_lgbm.npy\n",
      "LGBM training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------\n",
    "# NOTEBOOK 2: LGBM TRAINING & TEST PREDICTION\n",
    "# ----------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def feature_engineer(df):\n",
    "    \"\"\"Applies all feature engineering steps to the dataframe.\"\"\"\n",
    "    print(\"Applying feature engineering...\")\n",
    "    x_cols = [col for col in df.columns if col.startswith('X_')]\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    # Imbalances, Spreads, Sizes, Proxies, Logs, and basic interactions\n",
    "    df['order_book_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + epsilon)\n",
    "    df['trade_imbalance'] = (df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'] + epsilon)\n",
    "    df['quantity_spread'] = df['ask_qty'] - df['bid_qty']\n",
    "    df['avg_trade_size'] = df['volume'] / (df['buy_qty'] + df['sell_qty'] + epsilon)\n",
    "    df['wap_proxy'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + epsilon)\n",
    "    df['log_volume'] = np.log1p(df['volume'])\n",
    "    df['log_ask_qty'] = np.log1p(df['ask_qty'])\n",
    "    df['log_bid_qty'] = np.log1p(df['bid_qty'])\n",
    "    df['imbalance_times_volume'] = df['order_book_imbalance'] * df['volume']\n",
    "\n",
    "    # X-feature statistics\n",
    "    df['X_mean'] = df[x_cols].mean(axis=1)\n",
    "    df['X_std'] = df[x_cols].std(axis=1)\n",
    "    df['X_skew'] = df[x_cols].skew(axis=1)\n",
    "    df['X_median'] = df[x_cols].median(axis=1)\n",
    "    df['X_kurtosis'] = df[x_cols].kurtosis(axis=1)\n",
    "\n",
    "    # Advanced ratios and interactions\n",
    "    df['total_depth'] = df['bid_qty'] + df['ask_qty']\n",
    "    df['activity_intensity'] = df['volume'] / (df['total_depth'] + epsilon)\n",
    "    df['imbalance_delta'] = df['trade_imbalance'] - df['order_book_imbalance']\n",
    "\n",
    "    # Fill any potential NaNs created by division by zero\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    INPUT_X_PATH = '/kaggle/input/feature-engineering/X_engineered.parquet'\n",
    "    INPUT_Y_PATH = '/kaggle/input/feature-engineering/y_engineered.parquet'\n",
    "    DATA_PATH_TEST = '/kaggle/input/drw-crypto-market-prediction/test.parquet' # Raw test data\n",
    "    \n",
    "    OUTPUT_OOF_PATH = '/kaggle/working/oof_lgbm.npy'\n",
    "    OUTPUT_FEATURES_PATH = '/kaggle/working/sorted_features.csv'\n",
    "    OUTPUT_TEST_PREDS_PATH = '/kaggle/working/test_preds_lgbm.npy' # New output\n",
    "    OUTPUT_MODEL_DIR = '/kaggle/working/lgbm_models/' \n",
    "    N_SPLITS = 5\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create the output directory for models\n",
    "    os.makedirs(Config.OUTPUT_MODEL_DIR, exist_ok=True) # <-- ADD THIS\n",
    "\n",
    "    print(\"Loading engineered train data...\")\n",
    "    X = pd.read_parquet(Config.INPUT_X_PATH)\n",
    "    y = pd.read_parquet(Config.INPUT_Y_PATH)['label']\n",
    "    features = X.columns.tolist()\n",
    "    \n",
    "    print(\"Loading and engineering test data...\")\n",
    "    X_test = pd.read_parquet(Config.DATA_PATH_TEST)\n",
    "    X_test = feature_engineer(X_test)\n",
    "    X_test = X_test[features].astype(np.float32)\n",
    "\n",
    "    print(\"\\n--- Starting LightGBM Training & Prediction ---\")\n",
    "    \n",
    "    lgb_params = { \n",
    "        'objective': 'regression', 'metric': 'rmse', 'n_estimators': 3000, \n",
    "        'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 128,\n",
    "        'verbose': -1, 'n_jobs': -1, 'seed': Config.RANDOM_STATE, \n",
    "        'boosting_type': 'gbdt'\n",
    "    }\n",
    "    \n",
    "    oof_lgbm = np.zeros(len(X))\n",
    "    test_preds_lgbm = np.zeros(len(X_test))\n",
    "    feature_importances = pd.DataFrame(index=features)\n",
    "    kf = KFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"LGBM Fold {fold+1}/{Config.N_SPLITS}\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        \n",
    "        # --- SAVE THE FOLD MODEL ---\n",
    "        model_path = os.path.join(Config.OUTPUT_MODEL_DIR, f'lgbm_fold_{fold+1}.txt')\n",
    "        model.booster_.save_model(model_path)\n",
    "        print(f\"Saved model to {model_path}\")\n",
    "        # ---------------------------\n",
    "        \n",
    "        oof_lgbm[val_idx] = model.predict(X_val).clip(-5, 5)\n",
    "        test_preds_lgbm += model.predict(X_test).clip(-5, 5) / Config.N_SPLITS\n",
    "        \n",
    "        feature_importances[f'fold_{fold+1}'] = model.feature_importances_\n",
    "    \n",
    "    # --- Save all artifacts ---\n",
    "    feature_importances['mean'] = feature_importances.mean(axis=1)\n",
    "    lgbm_score = pearsonr(y, oof_lgbm)[0]\n",
    "    print(f\"\\nLGBM OOF Pearson Correlation: {lgbm_score:.5f}\")\n",
    "    \n",
    "    NUM_TOP_FEATURES = 250\n",
    "    sorted_features = feature_importances.sort_values('mean', ascending=False).index.tolist()\n",
    "    top_features = sorted_features[:NUM_TOP_FEATURES]\n",
    "\n",
    "    print(f\"Saving top {NUM_TOP_FEATURES} feature list to {Config.OUTPUT_FEATURES_PATH}\")\n",
    "    pd.Series(top_features).to_csv(Config.OUTPUT_FEATURES_PATH, index=False, header=False)\n",
    "    \n",
    "    print(f\"Saving LGBM OOF predictions to {Config.OUTPUT_OOF_PATH}\")\n",
    "    np.save(Config.OUTPUT_OOF_PATH, oof_lgbm)\n",
    "    \n",
    "    print(f\"Saving LGBM Test predictions to {Config.OUTPUT_TEST_PREDS_PATH}\")\n",
    "    np.save(Config.OUTPUT_TEST_PREDS_PATH, test_preds_lgbm)\n",
    "    \n",
    "    print(\"LGBM training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12993472,
     "sourceId": 96164,
     "sourceType": "competition"
    },
    {
     "sourceId": 250924561,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13824.085611,
   "end_time": "2025-07-17T14:10:16.927338",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-17T10:19:52.841727",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
